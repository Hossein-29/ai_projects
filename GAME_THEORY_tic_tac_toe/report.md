# گزارش تمرین سری چهارم عملی
# حسین بابازاده |‌ 401521066
## الگوریتم ها
- توضیح اولیه: به صورت کلی هر الگوریتم به این صورت پیاده سازی شده که به ازای یک state حرکت بعدی را مشخص میکند.
- الگوریتم minimax: این الگوریتم دقیقا همانند جزئیات تئوری آن پیاده سازی شده است و الگوریتم بهینه ای است. به عنوان مرجع به این الگوریتم نگاه میکنیم.
- الگوریتم alpha_beta pruning: این الگوریتم همان minimax است با این تفاوت که در مواردی که در جواب نهایی تاثیر ندارد بعضی از شاخه ها را در نظر نمیگیریم.(pruning)
- الگوریتم evaluation: ین الگوریتم هم مانند minimax است با این تفاوت که تا انتهای عمق درخت پیش نمیرویم و بعد از یک عمقی یک تخمینی از حالت ها در نظر میگیریم و طبق آن حرکت بعدی را تعیین میکنیم. در این تمرین به این صورت ارزش یک یک حالت را تعیین میکنیم:
    good_lines_turn * 1 - good_lines_nex_turn * 2
    به یک خط good_line میگوییم اگر بتوان یک خط (عمودی، افقی یا مورب) بتوان در جدول کشید که حداقل ۲ خانه از ۳ خانه آن از یک نوع باشد و خانه دیگر خالی باشد. که این مورد نشان دهنده این است که آن بازیکن به برد نزدیک است. حالا برای بهبود باید این را درنظر گرفت که نوبت حرکت کدوم بازیکن است پس به همین خاطر برای بازیکنی که نوبتش در دور بعدی است ضریب ۲ دارد.
- الگورتیم monte_carlo: این الگوریتم با ۳ الگوریتم قبلی تفاوت بیشتری دارد. به این صورت است که ما به ازای یک state حالت های مختلف را امتحان میکنیم و با توجه به نتایج این بازی ها تصمیم به حرکت بعد میکنیم. این الگوریتم نسخه های مختلفی دارد و ما در اینجا ساده ترین نسخه آن را پیاده سازی کردیم. به این صورت که از یک state به صورت random یکی از بچه ها را انتخاب میکنیم و همینطور به صورت random بازی کردن را ادامه میدهیم تا بازی تمام شود. بعد از اتمام بازی نتیجه را در راس فرزند state موردنظر ذخیره میکنیم بعد از تعدادی بار بازی(در اینجا ۳۰ بار بازی میکنیم) از بین بچه ها راسی را انتخاب میکنیم که بیشترین امتیاز را دارد.

## نتایج آزمایش ها
- نتایج آزمایش ها به کمک استخراج شده است(در فایل TicTacToe.ipynb) که به صورت زیر است:

| p1                            | p2                            | count | p1_win | p2_win | draw | avg1      | avg2      |
|-------------------------------|-------------------------------|-------|--------|--------|------|-----------|-----------|
| minimax                       | alpha_beta                    | 1     | 0      | 0      | 1    | 1.004     | 0.022     |
| evaluation_based              | minimax                       | 1     | 0      | 0      | 1    | 0.014     | 0.126     |
| minimax                       | monte_carlo_tree_search       | 30    | 24     | 0      | 6    | 1.402     | 0.002     |
| alpha_beta                    | evaluation_based              | 1     | 1      | 0      | 0    | 0.363     | 0.014     |
| monte_carlo_tree_search       | alpha_beta                    | 30    | 0      | 15     | 15   | 0.002     | 0.045     |
| evaluation_based              | monte_carlo_tree_search       | 30    | 26     | 0      | 4    | 0.018     | 0.002     |


## تحلیل نتایج
- عملکرد و تقاضای سخت افزاری دو الگوریتم minimax و هرس آلفا-بتا را مقایسه کنید.
    - به صورت تئوری این دو الگوریتم یک نتیجه دارند. همانطور که بالاتر گفته شد alpha-beta پیمایش هایی که در نتیجه تاثیری ندارند را انجام نمیدهد. ماهیت الگوریتم minimax همانند الگوریتم dfs است پس به لحاظ حافظه هر دو الگوریتم به اندازه عمق درخت(در این مسئله  ۹) می باشد و بین این دو الگوریتم تفاوتی وجود ندارد. اما به لحاظ زمانی alpha-beta میتواند بهتر عمل کند. اگر تحلیل اردر داشته باشیم جفت الگوریتم ها از یک مرتبه هستند، چون آلفا-بتا در بدترین حالت مانند minimax پیمایش میکند. در نظر بگیریم که عمق درخت m باشد پس اردر minimax از O(b^m) باشد، در این حالت الگوریتم alpha-beta در بهترین حالت از اردر O(b^(m/2)) خواهد بود. که چون تفاوت در توان است در عمل به مراتب قابل توجه است که این مورد در نتایج آزمایش ها هم مشهود است.

- یک الگوریتم تابع ارزیابی پیشنهاد دهید و عملکرد آن را با دو الگوریتم پیشین مقایسه کنید.
    - تابع ارزیابی در بخش توضیح الگوریتم ها شرح داده شد. از نتایج آزمایش ها میتوان دید که یکبار مقابل minimax مساوی کرده و یکبار هم در مقابل alpha-beta برده است.(تفاوت این بوده که یکبار به عنوان نفر اول بود و یکبار هم به عنوان نفر دوم و به همین دلیل نتایج مختلف داشتیم). به لحاظ زمانی اما بهتر از minimax و alpha-beta هم عمل کرده است.

- با توجه به اینکه minimax دقیق ترین الگوریتم است، حرکات الگوریتم های دیگر را با آن مقایسه کنید.
    - الگوریتم alpha-beta: دقت آن برابر با minimax است. یعنی بهینه ترین جواب را پیدا میکند.
    - الگوریتم evaluation: بسته به اینکه نحوه تخمین چطور باشد، میتواند دقیق باشد که در این صورت میتوان گفت دقت آن کمتر از minimax می باشد.
    - الگوریتم monte-carlo: هر چه بیشتر این الگوریتم امتحان کند دقت آن بیشتر میشود به طوری که دقت آن در بینهایت همانند minimax خواهد بود و راه بیهنیه را انتخاب میکند.

- نمودار میانگین زمان اجرا الگوریتم ها به صورت زیر است:
    ![average_execution_time](/game_theory_tic_tac_toe//avg_exec_time.png)
- همانطور که پیشتر توضیح داده شد این رفتار انتظار میرفت


- نمودار نرخ برد الگوریتم ها هم به شکل زیر است.
    ![win_ration](/game_theory_tic_tac_toe/win_ratio.png)
- این نمودار کمی عجیب به نظر میرسد با توضیحات قبلی چون نرخ برد evaluation بیشتر است ولی همه این موارد به این خاطر است که شرایط یکسان بین اجرای الگوریتم ها نبوده و نمیتوان گفت که تئوریی هایی که پیشتر گفته شد اشتباه بوده.

البته باید توجه داشت این آمار مربوط به اجرای همان حالت های خواسته شده در صورت تمرین است و به عنوان مثال اگر نرخ برد آلفا-بتا با مینیمکس برابر نیست به دلیل عدم تفارن بازی های انجام شده است.



